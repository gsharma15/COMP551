{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FR-Train on clean synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from argparse import Namespace\n",
    "\n",
    "from FRTrain_arch import Generator, DiscriminatorF, DiscriminatorR, weights_init_normal, test_model\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process data (using clean y train label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a namespace object which contains some of the hyperparameters\n",
    "opt = Namespace(num_train=2000, num_val1=200, num_val2=500, num_test=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = opt.num_train\n",
    "num_val1 = opt.num_val1\n",
    "num_val2 = opt.num_val2\n",
    "num_test = opt.num_test\n",
    "\n",
    "X = np.load('X_synthetic.npy') # Input features\n",
    "y = np.load('y_synthetic.npy') # Labels\n",
    "s1 = np.load('s1_synthetic.npy') # Sensitive features\n",
    "\n",
    "X = torch.FloatTensor(X)\n",
    "y = torch.FloatTensor(y)\n",
    "s1 = torch.FloatTensor(s1)\n",
    "\n",
    "X_train = X[:num_train - num_val1]\n",
    "y_train = y[:num_train - num_val1] # Clean label\n",
    "s1_train = s1[:num_train - num_val1]\n",
    "\n",
    "\n",
    "X_val = X[num_train: num_train + num_val1]\n",
    "y_val = y[num_train: num_train + num_val1]\n",
    "s1_val = s1[num_train: num_train + num_val1]\n",
    "\n",
    "# Currently not used\n",
    "# X_val2 = X[num_train + num_val1 : num_train + num_val1 + num_val2]\n",
    "# y_val2 = y[num_train + num_val1 : num_train + num_val1 + num_val2]\n",
    "# s1_val2 = s1[num_train + num_val1 : num_train + num_val1 + num_val2]\n",
    "\n",
    "X_test = X[num_train + num_val1 + num_val2 : num_train + num_val1 + num_val2 + num_test]\n",
    "y_test = y[num_train + num_val1 + num_val2 : num_train + num_val1 + num_val2 + num_test]\n",
    "s1_test = s1[num_train + num_val1 + num_val2 : num_train + num_val1 + num_val2 + num_test]\n",
    "\n",
    "XS_train = torch.cat([X_train, s1_train.reshape((s1_train.shape[0], 1))], dim=1)\n",
    "XS_val = torch.cat([X_val, s1_val.reshape((s1_val.shape[0], 1))], dim=1)\n",
    "XS_test = torch.cat([X_test, s1_test.reshape((s1_test.shape[0], 1))], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------- Number of Data -------------------------\n",
      "Train data : 1800, Validation data : 200, Test data : 1000 \n",
      "--------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------------- Number of Data -------------------------\" )\n",
    "print(\n",
    "    \"Train data : %d, Validation data : %d, Test data : %d \"\n",
    "    % (len(y_train), len(y_val), len(y_test))\n",
    ")       \n",
    "print(\"--------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_model(train_tensors, val_tensors, test_tensors, train_opt, lambda_f, lambda_r, seed):\n",
    "    \"\"\"\n",
    "      Trains FR-Train by using the classes in FRTrain_arch.py.\n",
    "      \n",
    "      Args:\n",
    "        train_tensors: Training data.\n",
    "        val_tensors: Clean validation data.\n",
    "        test_tensors: Test data.\n",
    "        train_opt: Options for the training. It currently contains size of validation set, \n",
    "                number of epochs, generator/discriminator update ratio, and learning rates.\n",
    "        lambda_f: The tuning knob for L_2 (ref: FR-Train paper, Section 3.3).\n",
    "        lambda_r: The tuning knob for L_3 (ref: FR-Train paper, Section 3.3).\n",
    "        seed: An integer value for specifying torch random seed.\n",
    "        \n",
    "      Returns:\n",
    "        Information about the tuning knobs (lambda_f, lambda_r),\n",
    "        the test accuracy of the trained model, and disparate impact of the trained model.\n",
    "    \"\"\"\n",
    "    \n",
    "    XS_train = train_tensors.XS_train\n",
    "    y_train = train_tensors.y_train\n",
    "    s1_train = train_tensors.s1_train\n",
    "    \n",
    "    XS_val = val_tensors.XS_val\n",
    "    y_val = val_tensors.y_val\n",
    "    s1_val = val_tensors.s1_val\n",
    "    \n",
    "    XS_test = test_tensors.XS_test\n",
    "    y_test = test_tensors.y_test\n",
    "    s1_test = test_tensors.s1_test\n",
    "    \n",
    "    # Saves return values here\n",
    "    test_result = []\n",
    "    \n",
    "    val = train_opt.val # Number of data points in validation set\n",
    "    k = train_opt.k     # Updates ratio of generator and discriminator (1:k training).\n",
    "    n_epochs = train_opt.n_epochs  # Number of training epoch\n",
    "    \n",
    "    # Changes the input validation data to an appropriate shape for the training\n",
    "    XSY_val = torch.cat([XS_val, y_val.reshape((y_val.shape[0], 1))], dim=1)  \n",
    "\n",
    "    # The loss values of each component will be saved in the following lists. \n",
    "    # We can draw epoch-loss graph by the following lists, if necessary.\n",
    "    g_losses =[]\n",
    "    d_f_losses = []\n",
    "    d_r_losses = []\n",
    "    clean_test_result = []\n",
    "\n",
    "    bce_loss = torch.nn.BCELoss()\n",
    "\n",
    "    # Initializes generator and discriminator\n",
    "    generator = Generator()\n",
    "    discriminator_F = DiscriminatorF()\n",
    "    discriminator_R = DiscriminatorR()\n",
    "\n",
    "    # Initializes weights\n",
    "    torch.manual_seed(seed)\n",
    "    generator.apply(weights_init_normal)\n",
    "    discriminator_F.apply(weights_init_normal)\n",
    "    discriminator_R.apply(weights_init_normal)\n",
    "\n",
    "    optimizer_G = torch.optim.Adam(generator.parameters(), lr=train_opt.lr_g)\n",
    "    optimizer_D_F = torch.optim.SGD(discriminator_F.parameters(), lr=train_opt.lr_f)\n",
    "    optimizer_D_R = torch.optim.SGD(discriminator_R.parameters(), lr=train_opt.lr_r)\n",
    "\n",
    "    XSY_val_data = XSY_val[:val]\n",
    "\n",
    "    train_len = XS_train.shape[0]\n",
    "    val_len = XSY_val.shape[0]\n",
    "\n",
    "    # Ground truths using in Disriminator_R\n",
    "    Tensor = torch.FloatTensor\n",
    "    valid = Variable(Tensor(train_len, 1).fill_(1.0), requires_grad=False)\n",
    "    generated = Variable(Tensor(train_len, 1).fill_(0.0), requires_grad=False)\n",
    "    fake = Variable(Tensor(train_len, 1).fill_(0.0), requires_grad=False)\n",
    "    clean = Variable(Tensor(val_len, 1).fill_(1.0), requires_grad=False)\n",
    "    \n",
    "    r_weight = torch.ones_like(y_train, requires_grad=False).float()\n",
    "    r_ones = torch.ones_like(y_train, requires_grad=False).float()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # -------------------\n",
    "        #  Forwards Generator\n",
    "        # -------------------\n",
    "        if epoch % k == 0 or epoch < 500:\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "        gen_y = generator(XS_train)\n",
    "        gen_data = torch.cat([XS_train, gen_y.detach().reshape((gen_y.shape[0], 1))], dim=1)\n",
    "        \n",
    "        # -----------------------------\n",
    "        #  Trains Fairness Discriminator\n",
    "        # -----------------------------\n",
    "\n",
    "        optimizer_D_F.zero_grad()\n",
    "\n",
    "        # Discriminator_F tries to distinguish the sensitive groups by using the output of the generator.\n",
    "        d_f_loss= bce_loss(discriminator_F(gen_y.detach()), s1_train)\n",
    "        d_f_loss.backward()\n",
    "        d_f_losses.append(d_f_loss)\n",
    "        optimizer_D_F.step()\n",
    "            \n",
    "\n",
    "        # ---------------------------------\n",
    "        #  Trains Robustness Discriminator\n",
    "        # ---------------------------------\n",
    "        optimizer_D_R.zero_grad()\n",
    "\n",
    "        # Discriminator_R tries to distinguish whether the input is from the validation data or the generated data from generator.\n",
    "        clean_loss = bce_loss(discriminator_R(XSY_val_data), clean)\n",
    "        poison_loss = bce_loss(discriminator_R(gen_data.detach()), fake)\n",
    "        d_r_loss = 0.5 * (clean_loss + poison_loss)\n",
    "\n",
    "        d_r_loss.backward()\n",
    "        d_r_losses.append(d_r_loss)\n",
    "        optimizer_D_R.step()\n",
    "\n",
    "\n",
    "        # ---------------------\n",
    "        #  Updates Generator\n",
    "        # ---------------------\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminators\n",
    "        if epoch < 500 :\n",
    "            g_loss = bce_loss((F.tanh(gen_y)+1)/2, (y_train+1)/2)\n",
    "            g_loss.backward()\n",
    "            g_losses.append(g_loss)\n",
    "            optimizer_G.step()\n",
    "    \n",
    "        elif epoch % k == 0:\n",
    "            r_decision = discriminator_R(gen_data)\n",
    "            r_gen = bce_loss(r_decision, generated)\n",
    "            \n",
    "            # ------------------------------\n",
    "            #  Re-weights using output of D_R\n",
    "            # ------------------------------\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                loss_ratio = (g_losses[-1]/d_r_losses[-1]).detach()\n",
    "                a = 1/(1+torch.exp(-(loss_ratio-3)))\n",
    "                b = 1-a\n",
    "                r_weight_tmp = r_decision.detach().squeeze()\n",
    "                r_weight = a * r_weight_tmp + b * r_ones\n",
    "\n",
    "            f_cost = F.binary_cross_entropy(discriminator_F(gen_y), s1_train, reduction=\"none\").squeeze()\n",
    "            g_cost = F.binary_cross_entropy_with_logits(gen_y.squeeze(), (y_train.squeeze()+1)/2, reduction=\"none\").squeeze()\n",
    "\n",
    "            f_gen = torch.mean(f_cost*r_weight)\n",
    "            g_loss = (1-lambda_f-lambda_r) * torch.mean(g_cost*r_weight) -  lambda_r * r_gen - lambda_f * f_gen\n",
    "\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "        g_losses.append(g_loss)\n",
    "\n",
    "        if epoch % 200 == 0:\n",
    "            print(\n",
    "                \"[Lambda_f: %1f] [Epoch %d/%d] [D_F loss: %f] [D_R loss: %f] [G loss: %f]\"\n",
    "                % (lambda_f, epoch, n_epochs, d_f_losses[-1], d_r_losses[-1], g_losses[-1])\n",
    "            )\n",
    "\n",
    "#     torch.save(generator.state_dict(), './FR-Train_on_clean_synthetic.pth')\n",
    "    tmp = test_model(generator, XS_test, y_test, s1_test)\n",
    "    test_result.append([lambda_f, lambda_r, tmp[0].item(), tmp[1]])\n",
    "\n",
    "    return test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_result = []\n",
    "train_tensors = Namespace(XS_train = XS_train, y_train = y_train, s1_train = s1_train)\n",
    "val_tensors = Namespace(XS_val = XS_val, y_val = y_val, s1_val = s1_val) \n",
    "test_tensors = Namespace(XS_test = XS_test, y_test = y_test, s1_test = s1_test)\n",
    "\n",
    "train_opt = Namespace(val=len(y_val), n_epochs=4000, k=3, lr_g=0.005, lr_f=0.01, lr_r=0.001)      \n",
    "seed = 1 \n",
    "\n",
    "lambda_f_set = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.85] # Lambda value for the fairness discriminator of FR-Train.\n",
    "lambda_r = 0.1 # Lambda value for the robustness discriminator of FR-Train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([1800])) that is different to the input size (torch.Size([1800, 1])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lambda_f \u001b[38;5;129;01min\u001b[39;00m lambda_f_set:\n\u001b[0;32m----> 2\u001b[0m     train_result\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_opt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_f\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlambda_f\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_r\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlambda_r\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(train_tensors, val_tensors, test_tensors, train_opt, lambda_f, lambda_r, seed)\u001b[0m\n\u001b[1;32m     96\u001b[0m optimizer_D_F\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# Discriminator_F tries to distinguish the sensitive groups by using the output of the generator.\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m d_f_loss\u001b[38;5;241m=\u001b[39m \u001b[43mbce_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiscriminator_F\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgen_y\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms1_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m d_f_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    101\u001b[0m d_f_losses\u001b[38;5;241m.\u001b[39mappend(d_f_loss)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mp4/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mp4/lib/python3.8/site-packages/torch/nn/modules/loss.py:612\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 612\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mp4/lib/python3.8/site-packages/torch/nn/functional.py:3056\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3054\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize():\n\u001b[0;32m-> 3056\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3057\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) is deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3058\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(target\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m   3059\u001b[0m     )\n\u001b[1;32m   3061\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3062\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n",
      "\u001b[0;31mValueError\u001b[0m: Using a target size (torch.Size([1800])) that is different to the input size (torch.Size([1800, 1])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "for lambda_f in lambda_f_set:\n",
    "    train_result.append(train_model(train_tensors, val_tensors, test_tensors, train_opt, lambda_f = lambda_f, lambda_r = lambda_r, seed=seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "------------------ Training Results of FR-Train on clean data ------------------\n",
      "[Lambda_f: 0.20] [Lambda_r: 0.10] Accuracy : 0.867, Disparate Impact : 0.492 \n",
      "[Lambda_f: 0.30] [Lambda_r: 0.10] Accuracy : 0.866, Disparate Impact : 0.528 \n",
      "[Lambda_f: 0.40] [Lambda_r: 0.10] Accuracy : 0.857, Disparate Impact : 0.554 \n",
      "[Lambda_f: 0.50] [Lambda_r: 0.10] Accuracy : 0.858, Disparate Impact : 0.608 \n",
      "[Lambda_f: 0.60] [Lambda_r: 0.10] Accuracy : 0.847, Disparate Impact : 0.663 \n",
      "[Lambda_f: 0.70] [Lambda_r: 0.10] Accuracy : 0.837, Disparate Impact : 0.711 \n",
      "[Lambda_f: 0.80] [Lambda_r: 0.10] Accuracy : 0.820, Disparate Impact : 0.766 \n",
      "[Lambda_f: 0.85] [Lambda_r: 0.10] Accuracy : 0.807, Disparate Impact : 0.818 \n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------------------------------------------------------------------------\")\n",
    "print(\"------------------ Training Results of FR-Train on clean data ------------------\" )\n",
    "for i in range(len(train_result)):\n",
    "    print(\n",
    "        \"[Lambda_f: %.2f] [Lambda_r: %.2f] Accuracy : %.3f, Disparate Impact : %.3f \"\n",
    "        % (train_result[i][0][0], train_result[i][0][1], train_result[i][0][2], train_result[i][0][3])\n",
    "    )       \n",
    "print(\"--------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
